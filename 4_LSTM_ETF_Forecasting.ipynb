{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRVfSaEYzVTp"
      },
      "source": [
        "# 1. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufjsmj49zm1v"
      },
      "source": [
        "1) 누적 거래량이 상위 30퍼인 ETF들에 대해 수익률 계산, 일주일 단위로 데이터를 묶음\n",
        "\n",
        "2) 일주일 단위로 묶인 데이터 내 각 컬럼의 최대, 최소, 평균값, 전주 대비 증감 가격(증감율), 수익률 계산\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5G0vPffF1Gx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import unicodedata\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbcZRjGZxj-E",
        "outputId": "e5a24fe3-bb3f-47af-c772-6f10259aa925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 199/1629 [00:58<03:01,  7.88it/s]<ipython-input-2-4bf420b4795e>:50: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  price_change_rate = (price_change / prev_close_price) * 100  # 전주대비 증감 비율\n",
            "100%|██████████| 1629/1629 [04:32<00:00,  5.97it/s]\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/NH_CONTEST_STK_DT_QUT_30.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# 'BSE_DT' 컬럼을 날짜 형식으로 변환\n",
        "df['bse_dt'] = pd.to_datetime(df['bse_dt'], format='%Y%m%d')\n",
        "# 'BSE_DT' 컬럼을 일주일 단위로 변환하여 'Week' 컬럼을 생성\n",
        "df['Week'] = df['bse_dt'].dt.to_period('W')\n",
        "\n",
        "df_sorted = df.sort_values(by=['Week', 'tck_iem_cd', 'bse_dt'])\n",
        "\n",
        "# 'tck_iem_cd'별로 데이터를 묶어 처리\n",
        "grouped = df_sorted.groupby('tck_iem_cd')\n",
        "\n",
        "output_rows = []\n",
        "\n",
        "# tck_iem_cd별로 데이터를 일주일 단위로 묶어 처리\n",
        "for tck_iem_cd, data in tqdm(grouped):\n",
        "    week_grouped = data.groupby('Week')\n",
        "\n",
        "    for week, week_data in week_grouped:\n",
        "        week_data = week_data.sort_values(by='bse_dt')  # 주간 데이터 정렬\n",
        "\n",
        "        # 주간 요약 정보\n",
        "        open_price = week_data.iloc[0]['iem_ong_pr']  # 종목 시가 (첫 날)\n",
        "        close_price = week_data.iloc[-1]['iem_end_pr']  # 종목 종가 (마지막 날)\n",
        "        high_price = week_data['iem_hi_pr'].max()  # 주간 고가\n",
        "        low_price = week_data['iem_low_pr'].min()  # 주간 저가\n",
        "        volume_high = week_data['acl_trd_qty'].max()  # 주간 거래량 최대\n",
        "        volume_low = week_data['acl_trd_qty'].min()  # 주간 거래량 최소\n",
        "        volume_mean = week_data['acl_trd_qty'].mean()  # 주간 거래량 평균\n",
        "        cost_high = week_data['trd_cst'].max()  # 거래대금 최대\n",
        "        cost_low = week_data['trd_cst'].min()  # 거래대금 최소\n",
        "        cost_mean = week_data['trd_cst'].mean()  # 거래대금 평균\n",
        "        sll_high = week_data['sll_cns_sum_qty'].max()  # 매도체결합계수량 최대\n",
        "        sll_low = week_data['sll_cns_sum_qty'].min()  # 매도체결합계수량 최소\n",
        "        sll_mean = week_data['sll_cns_sum_qty'].mean()  # 매도체결합계수량 평균\n",
        "        byn_high = week_data['byn_cns_sum_qty'].max()  # 매수체결합계수량 최대\n",
        "        byn_low = week_data['byn_cns_sum_qty'].min()  # 매수체결합계수량 최소\n",
        "        byn_mean = week_data['byn_cns_sum_qty'].mean()  # 매수체결합계수량 평균\n",
        "        rt_high = week_data['sby_bse_xcg_rt'].max()  # 환율 최대\n",
        "        rt_low = week_data['sby_bse_xcg_rt'].min()  # 환율 최소\n",
        "        rt_mean = week_data['sby_bse_xcg_rt'].mean()  # 환율 평균\n",
        "        yield_rate = (close_price - open_price) / open_price * 100 # 수익률\n",
        "\n",
        "        # 전주대비 증감 가격과 증감율\n",
        "        prev_week_data = df_sorted[(df_sorted['Week'] == week - 1) & (df_sorted['tck_iem_cd'] == tck_iem_cd)]\n",
        "        if not prev_week_data.empty:\n",
        "            prev_close_price = prev_week_data.iloc[-1]['iem_end_pr']\n",
        "            price_change = close_price - prev_close_price  # 전주대비 증감 가격\n",
        "            price_change_rate = (price_change / prev_close_price) * 100  # 전주대비 증감 비율\n",
        "        else:\n",
        "            price_change = None\n",
        "            price_change_rate = None\n",
        "\n",
        "        # 출력 리스트에 데이터 추가\n",
        "        output_rows.append({\n",
        "            'ETF_tck_name': tck_iem_cd,\n",
        "            'week': f'Week: {str(week)}',\n",
        "            '종목시가': open_price,\n",
        "            '종목종가': close_price,\n",
        "            '종목고가': high_price,\n",
        "            '종목저가': low_price,\n",
        "            '전주대비증감가격': price_change,\n",
        "            '전주대비증감률': price_change_rate,\n",
        "            '누적거래수량(고)': volume_high,\n",
        "            '누적거래수량(저)': volume_low,\n",
        "            '누적거래수량(평균)': volume_mean,\n",
        "            '거래대금(고)': cost_high,\n",
        "            '거래대금(저)': cost_low,\n",
        "            '거래대금(평균)': cost_mean,\n",
        "            '매도수량(고)': sll_high,\n",
        "            '매도수량(저)': sll_low,\n",
        "            '매도수량(평균)': sll_mean,\n",
        "            '매수수량(고)': byn_high,\n",
        "            '매수수량(저)': byn_low,\n",
        "            '매수수량(평균)': byn_mean,\n",
        "            '환율(고)': rt_high,\n",
        "            '환율(저)': rt_low,\n",
        "            '환율(평균)': rt_mean,\n",
        "            '수익률': yield_rate\n",
        "        })\n",
        "\n",
        "df_output = pd.DataFrame(output_rows)\n",
        "\n",
        "df_output.to_csv('/content/drive/MyDrive/grouped_by_week_etf.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQM1B3_b1PK4"
      },
      "source": [
        "3)학습/평가 데이터 분리\n",
        "\n",
        "- '2024-08-19/2024-08-25'주차의 데이터를 평가 데이터로 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrbiWygC0Cci",
        "outputId": "7b642549-9702-4e9f-dc40-fbd700572e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETF_tck_name\n",
            "AAN             14\n",
            "OFG             14\n",
            "ONTF            14\n",
            "ONL             14\n",
            "ONEW            14\n",
            "                ..\n",
            "FCFS            14\n",
            "ZYXI            14\n",
            "RSSL            13\n",
            "ONIT            12\n",
            "KCSH             6\n",
            "Name: count, Length: 1629, dtype: int64\n",
            "ETF_tck_name\n",
            "AAN             1\n",
            "OFLX            1\n",
            "ONTO            1\n",
            "ONTF            1\n",
            "ONL             1\n",
            "               ..\n",
            "FCPT            1\n",
            "FCNCA           1\n",
            "FCN             1\n",
            "FCFS            1\n",
            "ZYXI            1\n",
            "Name: count, Length: 1629, dtype: int64\n",
            "ETF_tck_name\n",
            "AAN             12\n",
            "OFG             12\n",
            "ONTF            12\n",
            "ONL             12\n",
            "ONEW            12\n",
            "                ..\n",
            "FCFS            12\n",
            "ZYXI            12\n",
            "RSSL            11\n",
            "ONIT            10\n",
            "KCSH             4\n",
            "Name: count, Length: 1629, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# train/test data 분리\n",
        "path = '/content/drive/MyDrive/grouped_by_week_etf.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# total 데이터 확인\n",
        "print(df['ETF_tck_name'].value_counts()) # 1629\n",
        "\n",
        "# 'Week: 2024-08-26/2024-09-01'을 제거\n",
        "df_train = df[df['week'] != 'Week: 2024-08-26/2024-09-01']\n",
        "\n",
        "# 'Week: 2024-08-19/2024-08-25'인 행을 테스트 데이터로 분리\n",
        "df_test = df_train[df_train['week'] == 'Week: 2024-08-19/2024-08-25']\n",
        "\n",
        "# Test 데이터에서 해당 행을 제거한 나머지를 Train 데이터로 사용\n",
        "df_train = df_train[df_train['week'] != 'Week: 2024-08-19/2024-08-25']\n",
        "\n",
        "# 분리된 데이터 확인\n",
        "print(df_test['ETF_tck_name'].value_counts()) # 1629\n",
        "print(df_train['ETF_tck_name'].value_counts()) # 1629\n",
        "\n",
        "# 각각의 데이터셋을 CSV 파일로 저장\n",
        "df_train.to_csv('/content/drive/MyDrive/total_train_data.csv', index=False)\n",
        "df_test.to_csv('/content/drive/MyDrive/total_test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmkSMN_52sit"
      },
      "source": [
        "# 2. 데이터셋\n",
        "- 모델의 입력인 텐서 형태가 되도록 데이터를 불러오는 역할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBXpq_ju2gEj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsrbnj1-21BN"
      },
      "outputs": [],
      "source": [
        "class NHDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, phase):\n",
        "        self.phase = phase\n",
        "\n",
        "        path = f'/content/drive/MyDrive/total_{self.phase}_data.csv'\n",
        "        df = pd.read_csv(path)\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)  # inf 값을 NaN으로 변환\n",
        "        df = df.dropna()\n",
        "        self.df = df\n",
        "\n",
        "        # ETF 이름 encoding\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.df['ETF_tck_name_encoded'] = self.label_encoder.fit_transform(df['ETF_tck_name'])\n",
        "\n",
        "        self.names = df['ETF_tck_name'].values\n",
        "        self.encoded_names = df['ETF_tck_name_encoded'].values\n",
        "        self.weeks = df['week'].values\n",
        "\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.scaled_etf = self.scaler.fit_transform(df.drop(['week', 'ETF_tck_name'], axis=1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # ETF 이름을 숫자로 인코딩한 값 추가\n",
        "        etf_encoded = self.encoded_names[index]\n",
        "        x = self.scaled_etf[index, :-1]  # features\n",
        "        y = self.scaled_etf[index, -1]   # ETF 수익률\n",
        "\n",
        "        # ETF 이름 + features\n",
        "        x_with_etf = np.append(x, etf_encoded)\n",
        "\n",
        "        x_tensor = torch.tensor(x_with_etf, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "        input_dic = {'ETF_tck_name': self.names[index],\n",
        "                     'week': self.weeks[index],\n",
        "                     'input': x_tensor,\n",
        "                     'label': y_tensor\n",
        "        }\n",
        "\n",
        "        return input_dic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBJvgYQ_3STf"
      },
      "source": [
        "# 3. 모델링\n",
        "\n",
        "- LSTM 기반 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS906eCs3RMy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCAfRZmB3YAK"
      },
      "outputs": [],
      "source": [
        "class ETFPredictionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ETFPredictionModel, self).__init__()\n",
        "\n",
        "        self.input_size = 23 # 하드 코딩 (# of features)\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 4\n",
        "\n",
        "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden = (\n",
        "                torch.zeros(self.layers, self.seq_len, self.hidden_dim),\n",
        "                torch.zeros(self.layers, self.seq_len, self.hidden_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # (batch, 1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5akpry43nqX"
      },
      "source": [
        "# 4. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sCaEatF3nFZ"
      },
      "outputs": [],
      "source": [
        "# gpu\n",
        "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8RJAnnr37Ns"
      },
      "outputs": [],
      "source": [
        "lr = 1e-2\n",
        "batch_size = 16\n",
        "log_dir = '/content/drive/MyDrive'\n",
        "ckpt_dir = '/content/drive/MyDrive/NH_model'\n",
        "\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9A62Qxw3yhb",
        "outputId": "cf500469-42d6-47dd-cb78-cb346726c50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "## Dataloader ##\n",
        "# 데이터셋 생성\n",
        "train_dataset = NHDataset(phase='train')\n",
        "loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "## model ##\n",
        "model = ETFPredictionModel()\n",
        "model.to(device)\n",
        "\n",
        "## optimization ##\n",
        "criterion = nn.MSELoss() # loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib-msD6Z322N"
      },
      "outputs": [],
      "source": [
        "st_epoch = 0\n",
        "num_epoch = 50\n",
        "best_val_accuracy = 0.0\n",
        "best_loss = float('inf')  # 가장 좋은 loss를 추적\n",
        "patience = 10  # early stopping을 위한 patience\n",
        "verbose = 10\n",
        "counter = 0  # patience 확인을 위한 카운터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh11rdKZ4ZQp",
        "outputId": "84154fc6-3c5c-45ac-e501-821fbe79cb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [00:07<00:00, 145.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model - epoch : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [00:08<00:00, 135.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model - epoch : 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [00:07<00:00, 154.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model - epoch : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [00:08<00:00, 134.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model - epoch : 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [00:09<00:00, 116.72it/s]\n",
            "100%|██████████| 1120/1120 [00:08<00:00, 139.08it/s]\n",
            "100%|██████████| 1120/1120 [00:10<00:00, 111.54it/s]\n",
            "100%|██████████| 1120/1120 [00:06<00:00, 169.84it/s]\n",
            "100%|██████████| 1120/1120 [00:08<00:00, 128.41it/s]\n",
            "100%|██████████| 1120/1120 [00:06<00:00, 165.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: EPOCH 0010/0050 | LOSS 0.0844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [00:08<00:00, 128.50it/s]\n",
            "100%|██████████| 1120/1120 [00:07<00:00, 155.06it/s]\n",
            "100%|██████████| 1120/1120 [00:08<00:00, 128.73it/s]\n",
            "100%|██████████| 1120/1120 [00:06<00:00, 167.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early Stopping at epoch 14 | Best Loss: 0.0120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(st_epoch+1, num_epoch+1):\n",
        "    # ---------------------------------- Training ---------------------------------- #\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "\n",
        "    for i, data in enumerate(tqdm(loader_train, 0)):\n",
        "        # forward pass\n",
        "        name = data['ETF_tck_name']\n",
        "        input, label = data['input'].to(device), data['label'].to(device)\n",
        "        label = label.view(-1, 1)  # (batch, 1)\n",
        "\n",
        "        input = input.unsqueeze(1)\n",
        "        output = model(input)\n",
        "\n",
        "        # backward pass\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    train_loss = np.mean(loss_list)\n",
        "\n",
        "    if epoch % verbose == 0:\n",
        "        print('TRAIN: EPOCH %04d/%04d | LOSS %.4f' % (epoch, num_epoch, train_loss))\n",
        "\n",
        "    # Early stopping\n",
        "    if train_loss < best_loss:  # 현재 epoch의 loss가 이전까지의 가장 좋은 loss보다 낮은 경우\n",
        "        best_loss = train_loss\n",
        "        counter = 0  # 카운터 초기화\n",
        "        torch.save(model.state_dict(), f'{ckpt_dir}/{epoch}.pth')\n",
        "        print(f'Saving the model - epoch : {epoch}')\n",
        "    else:\n",
        "        counter += 1  # loss가 개선되지 않으면 카운터 증가\n",
        "        if counter >= patience:\n",
        "            print(f'\\nEarly Stopping at epoch {epoch} | Best Loss: {best_loss:.4f}')\n",
        "            break  # patience 만큼 epoch 동안 loss 개선이 없으면 학습 중단\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQupvSDZ6Bch"
      },
      "source": [
        "# 5. 모델 추론 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRPxWrhD4_PT",
        "outputId": "456bc2d5-329c-4ffc-8196-0a93022f201c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "test_dataset = NHDataset(phase='test')\n",
        "loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXgWBtNg6QcJ",
        "outputId": "95e2f1f9-f023-4e3d-bc0b-3b4250d29128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-6acbd76331b2>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(load_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# 가장 마지막으로 저장된 epoch\n",
        "load_path = '/content/drive/MyDrive/NH_model/4.pth'\n",
        "model.load_state_dict(torch.load(load_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51tJjYtp6QtJ"
      },
      "outputs": [],
      "source": [
        "# 평가 metric\n",
        "def MAE(true, pred):\n",
        "    return np.mean(np.abs(true - pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeGddHqj6bM9"
      },
      "outputs": [],
      "source": [
        "pred, true = [], []\n",
        "result, etf = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP8NvrWi6beR",
        "outputId": "9b58729f-e1d6-4d1a-def8-bd15d723ebf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:01<00:00, 67.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.3102095425128937\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------- Inference ---------------------------------- #\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(tqdm(loader_test, 0)):\n",
        "    # forward pass\n",
        "    etf_tck_name, week = data['ETF_tck_name'], data['week']\n",
        "    input, label = data['input'].to(device), data['label'].to(device)\n",
        "    input = input.unsqueeze(1)\n",
        "\n",
        "    output = model(input)\n",
        "\n",
        "    b, _, _ = input.shape\n",
        "\n",
        "    for idx in range(b):\n",
        "        etf.append((etf_tck_name[idx], week[idx], output[idx].item()))\n",
        "\n",
        "    pred.extend(output.detach().cpu().numpy())\n",
        "    true.extend(label.detach().cpu().numpy())\n",
        "\n",
        "pred = np.array(pred)\n",
        "true = np.array(true)\n",
        "\n",
        "# 모델 평가 결과 출력 및 저장\n",
        "mae_score = MAE(true, pred)\n",
        "result.append(mae_score.item())\n",
        "print(f\"MAE: {mae_score}\")\n",
        "header_result= ['MAE']\n",
        "mae_score_df = pd.DataFrame(result)\n",
        "mae_score_df.to_csv(f'{ckpt_dir}/result.csv', index=False, header=header_result)\n",
        "\n",
        "# 모델 예측 결과 저장\n",
        "header_etf = ['ETF_tck_name', 'week', 'pred']\n",
        "etf_df = pd.DataFrame(etf)\n",
        "etf_df.to_csv(f'{ckpt_dir}/output.csv', index=False, header=header_etf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfQvAZRD6bqq",
        "outputId": "3ca14e06-0c36-4293-eadc-edf0bcdaa12b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETF 이름: PROK, 예측된 수익률: 0.633\n",
            "ETF 이름: BUCK, 예측된 수익률: 0.125\n",
            "ETF 이름: HYLN, 예측된 수익률: 0.365\n"
          ]
        }
      ],
      "source": [
        "# 모델 예측 결과 확인\n",
        "output_path = '/content/drive/MyDrive/NH_model/output.csv'\n",
        "output_df = pd.read_csv(output_path)\n",
        "\n",
        "pred_PROK = round(output_df[output_df['ETF_tck_name'] == 'PROK']['pred'].values[0], 3)\n",
        "print(f'ETF 이름: PROK, 예측된 수익률: {pred_PROK}')\n",
        "pred_BUCK = round(output_df[output_df['ETF_tck_name'] == 'BUCK']['pred'].values[0], 3)\n",
        "print(f'ETF 이름: BUCK, 예측된 수익률: {pred_BUCK}')\n",
        "pred_HYLN = round(output_df[output_df['ETF_tck_name'] == 'HYLN']['pred'].values[0], 3)\n",
        "print(f'ETF 이름: HYLN, 예측된 수익률: {pred_HYLN}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bYknjCo_uQk"
      },
      "source": [
        "# 6. 생성형 AI를 활용한 ETF 추천"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYZEsun3AUQb"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43NEvljxCeof"
      },
      "outputs": [],
      "source": [
        "corr_path = '/content/drive/MyDrive/상관계수추가_최종교집합ETF.csv'\n",
        "corr_df = pd.read_csv(corr_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMqzLal4C5Bl",
        "outputId": "68bd1ef5-391c-44dd-8a25-2ed530154c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    HYLN\n",
            "1    MOND\n",
            "2    RDVT\n",
            "Name: inter_금_pos_7, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# NaN이 아닌 값들만 출력\n",
        "non_nan_values = corr_df['inter_금_pos_7'].dropna()\n",
        "print(non_nan_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ig3EPZRAUFj"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = 'AIzaSyCorQViKYbN59xDJ7vtwVaeopFHI0OR2Bk'\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Set up the model\n",
        "generation_config = {\n",
        "  \"temperature\": 0.9,\n",
        "  \"top_p\": 1,\n",
        "  \"top_k\": 1,\n",
        "  \"max_output_tokens\": 1024,\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro',\n",
        "                             generation_config=generation_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"아래 예시를 참고해서, 금의 상승세와 밀접하게 관련있는 ETF 종목을 추천해줘. \\n 예시: - 종목: HYLN - 형식: 금의 상승세와 가장 밀접하게 관련이 있는 종목은 HYLN이며, 이 종목은 적극 투자 권유합니다.\\n - 종목: {non_nan_values[1]}\""
      ],
      "metadata": {
        "id": "7fIgwHElPNdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M_VwdSdPPT6w",
        "outputId": "2959620b-2bd4-42e2-a36d-1f5339773c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 종목: GDX - 형식: 금의 상승세와 밀접하게 관련이 있는 ETF 중에서 가장 주목해야 할 종목은 GDX이며, 이 종목은 적극 투자 권유합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23V8DOB4LdNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}